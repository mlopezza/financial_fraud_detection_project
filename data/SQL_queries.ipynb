{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c420dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Downloading duckdb-1.4.3-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n",
      "Downloading duckdb-1.4.3-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.3 MB 2.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/12.3 MB 2.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.1/12.3 MB 2.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.6/12.3 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/12.3 MB 3.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.5/12.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.5/12.3 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.6/12.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-1.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d4754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import duckdb\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b8c39",
   "metadata": {},
   "source": [
    "### Use the following bash command to find the cached file path\n",
    "find ~ -name \"financial_fraud_detection_dataset.csv\"\n",
    "\n",
    "If you're not able to, then download a copy of the dataset to your machine, unzip it and set the absolute path of the csv file in the next cell.\n",
    "\n",
    "- Download link:\n",
    "https://www.kaggle.com/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb2e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id                   timestamp sender_account receiver_account  \\\n",
      "0        T100000  2023-08-22T09:22:43.516168      ACC877572        ACC388389   \n",
      "1        T100001  2023-08-04T01:58:02.606711      ACC895667        ACC944962   \n",
      "2        T100002  2023-05-12T11:39:33.742963      ACC733052        ACC377370   \n",
      "3        T100003  2023-10-10T06:04:43.195112      ACC996865        ACC344098   \n",
      "4        T100004  2023-09-24T08:09:02.700162      ACC584714        ACC497887   \n",
      "\n",
      "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
      "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
      "1   419.65       withdrawal            online  Toronto         atm     False   \n",
      "2  2773.86          deposit             other   London         pos     False   \n",
      "3  1666.22          deposit            online   Sydney         pos     False   \n",
      "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
      "\n",
      "  fraud_type  time_since_last_transaction  spending_deviation_score  \\\n",
      "0        NaN                          NaN                     -0.21   \n",
      "1        NaN                          NaN                     -0.14   \n",
      "2        NaN                          NaN                     -1.78   \n",
      "3        NaN                          NaN                     -0.60   \n",
      "4        NaN                          NaN                      0.79   \n",
      "\n",
      "   velocity_score  geo_anomaly_score payment_channel       ip_address  \\\n",
      "0               3               0.22            card   13.101.214.112   \n",
      "1               7               0.96             ACH    172.52.47.194   \n",
      "2              20               0.89            card     185.98.35.23   \n",
      "3               6               0.37   wire_transfer    107.136.36.87   \n",
      "4              13               0.27             ACH  108.161.108.255   \n",
      "\n",
      "  device_hash  \n",
      "0    D8536477  \n",
      "1    D2622631  \n",
      "2    D4823498  \n",
      "3    D9961380  \n",
      "4    D7637601  \n"
     ]
    }
   ],
   "source": [
    "# Copy and paste the file path of the cached dataset below to read it into a pandas DataFrame\n",
    "df = pd.read_csv(\"/Users/User/.cache/kagglehub/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/versions/1/financial_fraud_detection_dataset.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19739dd1",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleaning Using SQL Queries in DuckDB\n",
    "\n",
    "METHOD:\n",
    "- Using a Local SQL Engine (DuckDB)\n",
    "    - For complex SQL queries, loading our data into a local analytical database like DuckDB is very effective. It's fast and supports direct querying on Pandas DataFrames or CSV files.\n",
    "    - We can use DuckDB to query CSV/parquet file directly and perform the filtering in SQL, which is more memory efficient.\n",
    "    - DuckDB is optimized for analytical queries and can be faster than pandas for complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea310a",
   "metadata": {},
   "source": [
    "   ** Workflow**\n",
    "   - \n",
    "    - Download dataset to local machine\n",
    "    - connect to path in jupyter notebook, and convert csv to parquet files (columnar Parquet files that are much faster to query)\n",
    "    - Store parquet files in folder within the repo (parquet files are smaller)\n",
    "    - Run sql queries directly on he parquet files without importing them into memory\n",
    "    - Feature Engineering (DuckDB SQL) or Pandas\n",
    "    - Saved clean and processed parquet shards/files to be used in other notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7536a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Source CSV: /Users/User/.cache/kagglehub/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/versions/1/financial_fraud_detection_dataset.csv\n",
      "üìÅ Target Parquet: ./raw_data/financial_fraud_detection_dataset.parquet\n",
      "üìä Original size: 0.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "csv_path = \"/Users/User/.cache/kagglehub/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/versions/1/financial_fraud_detection_dataset.csv\"\n",
    "parquet_path = \"./raw_data/financial_fraud_detection_dataset.parquet\"\n",
    "cleaned_parquet_path = \"./cleaned_data/cleaned_fraud.parquet\"\n",
    "\n",
    "# 1. Check if source CSV exists\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {csv_path}\")\n",
    "\n",
    "print(f\"üìÅ Source CSV: {csv_path}\")\n",
    "print(f\"üìÅ Target Parquet: {parquet_path}\")\n",
    "print(f\"üìä Original size: {os.path.getsize(csv_path) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16524a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Parquet size: 0.19 GB\n",
      "üéØ Compression ratio: 74.6% reduction\n",
      "üî¢ Row count in Parquet: 5,000,000\n"
     ]
    }
   ],
   "source": [
    "# Connect to duckdb and Convert CSV to Parquet with DuckDB\n",
    "\n",
    "# CREATE THE DIRECTORY parquet_path directory FIRST\n",
    "os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "\n",
    "# f used to pass in a variable to print or SQL statements\n",
    "# \"\"\" are used for multi-line SQL queries, \" is used for single-line SQL queries\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"\"\"\n",
    "COPY (SELECT * FROM read_csv_auto('{csv_path}'))\n",
    "TO '{parquet_path}' (FORMAT 'parquet', COMPRESSION 'zstd');\n",
    "\"\"\")\n",
    "\n",
    "#  3. Verify the result\n",
    "if os.path.exists(parquet_path):\n",
    "    parquet_size = os.path.getsize(parquet_path) / (1024**3)\n",
    "    compression_ratio = (1 - parquet_size / (os.path.getsize(csv_path) / (1024**3))) * 100\n",
    "    print(f\"üìä Parquet size: {parquet_size:.2f} GB\")\n",
    "    print(f\"üéØ Compression ratio: {compression_ratio:.1f}% reduction\")\n",
    "    \n",
    "    # Quick verification query\n",
    "    row_count = con.execute(f\"SELECT COUNT(*) FROM '{parquet_path}'\").fetchone()[0]\n",
    "    print(f\"üî¢ Row count in Parquet: {row_count:,}\")\n",
    "else:\n",
    "    print(\"‚ùå Parquet file was not created\")\n",
    "\n",
    "# To Close DB connection, but can be left open for further queries\n",
    "# con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41cfee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    column_name column_type null   key default extra\n",
      "0                transaction_id     VARCHAR  YES  None    None  None\n",
      "1                     timestamp   TIMESTAMP  YES  None    None  None\n",
      "2                sender_account     VARCHAR  YES  None    None  None\n",
      "3              receiver_account     VARCHAR  YES  None    None  None\n",
      "4                        amount      DOUBLE  YES  None    None  None\n",
      "5              transaction_type     VARCHAR  YES  None    None  None\n",
      "6             merchant_category     VARCHAR  YES  None    None  None\n",
      "7                      location     VARCHAR  YES  None    None  None\n",
      "8                   device_used     VARCHAR  YES  None    None  None\n",
      "9                      is_fraud     BOOLEAN  YES  None    None  None\n",
      "10                   fraud_type     VARCHAR  YES  None    None  None\n",
      "11  time_since_last_transaction      DOUBLE  YES  None    None  None\n",
      "12     spending_deviation_score      DOUBLE  YES  None    None  None\n",
      "13               velocity_score      BIGINT  YES  None    None  None\n",
      "14            geo_anomaly_score      DOUBLE  YES  None    None  None\n",
      "15              payment_channel     VARCHAR  YES  None    None  None\n",
      "16                   ip_address     VARCHAR  YES  None    None  None\n",
      "17                  device_hash     VARCHAR  YES  None    None  None\n"
     ]
    }
   ],
   "source": [
    "# con = duckdb.connect()\n",
    "\n",
    "# using DESCRIBE instead of pandas dtypes to avoid loading data into memory\n",
    "print(con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{parquet_path}')\").fetch_df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d284fa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sender_account</th>\n",
       "      <th>receiver_account</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>merchant_category</th>\n",
       "      <th>location</th>\n",
       "      <th>device_used</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>fraud_type</th>\n",
       "      <th>time_since_last_transaction</th>\n",
       "      <th>spending_deviation_score</th>\n",
       "      <th>velocity_score</th>\n",
       "      <th>geo_anomaly_score</th>\n",
       "      <th>payment_channel</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>device_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T100000</td>\n",
       "      <td>2023-08-22 09:22:43.516168</td>\n",
       "      <td>ACC877572</td>\n",
       "      <td>ACC388389</td>\n",
       "      <td>343.78</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>utilities</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>mobile</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>card</td>\n",
       "      <td>13.101.214.112</td>\n",
       "      <td>D8536477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T100001</td>\n",
       "      <td>2023-08-04 01:58:02.606711</td>\n",
       "      <td>ACC895667</td>\n",
       "      <td>ACC944962</td>\n",
       "      <td>419.65</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>online</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>atm</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>7</td>\n",
       "      <td>0.96</td>\n",
       "      <td>ACH</td>\n",
       "      <td>172.52.47.194</td>\n",
       "      <td>D2622631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T100002</td>\n",
       "      <td>2023-05-12 11:39:33.742963</td>\n",
       "      <td>ACC733052</td>\n",
       "      <td>ACC377370</td>\n",
       "      <td>2773.86</td>\n",
       "      <td>deposit</td>\n",
       "      <td>other</td>\n",
       "      <td>London</td>\n",
       "      <td>pos</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>20</td>\n",
       "      <td>0.89</td>\n",
       "      <td>card</td>\n",
       "      <td>185.98.35.23</td>\n",
       "      <td>D4823498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T100003</td>\n",
       "      <td>2023-10-10 06:04:43.195112</td>\n",
       "      <td>ACC996865</td>\n",
       "      <td>ACC344098</td>\n",
       "      <td>1666.22</td>\n",
       "      <td>deposit</td>\n",
       "      <td>online</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>pos</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>6</td>\n",
       "      <td>0.37</td>\n",
       "      <td>wire_transfer</td>\n",
       "      <td>107.136.36.87</td>\n",
       "      <td>D9961380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T100004</td>\n",
       "      <td>2023-09-24 08:09:02.700162</td>\n",
       "      <td>ACC584714</td>\n",
       "      <td>ACC497887</td>\n",
       "      <td>24.43</td>\n",
       "      <td>transfer</td>\n",
       "      <td>utilities</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>mobile</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>13</td>\n",
       "      <td>0.27</td>\n",
       "      <td>ACH</td>\n",
       "      <td>108.161.108.255</td>\n",
       "      <td>D7637601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id                  timestamp sender_account receiver_account  \\\n",
       "0        T100000 2023-08-22 09:22:43.516168      ACC877572        ACC388389   \n",
       "1        T100001 2023-08-04 01:58:02.606711      ACC895667        ACC944962   \n",
       "2        T100002 2023-05-12 11:39:33.742963      ACC733052        ACC377370   \n",
       "3        T100003 2023-10-10 06:04:43.195112      ACC996865        ACC344098   \n",
       "4        T100004 2023-09-24 08:09:02.700162      ACC584714        ACC497887   \n",
       "\n",
       "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
       "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
       "1   419.65       withdrawal            online  Toronto         atm     False   \n",
       "2  2773.86          deposit             other   London         pos     False   \n",
       "3  1666.22          deposit            online   Sydney         pos     False   \n",
       "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
       "\n",
       "  fraud_type  time_since_last_transaction  spending_deviation_score  \\\n",
       "0       None                          NaN                     -0.21   \n",
       "1       None                          NaN                     -0.14   \n",
       "2       None                          NaN                     -1.78   \n",
       "3       None                          NaN                     -0.60   \n",
       "4       None                          NaN                      0.79   \n",
       "\n",
       "   velocity_score  geo_anomaly_score payment_channel       ip_address  \\\n",
       "0               3               0.22            card   13.101.214.112   \n",
       "1               7               0.96             ACH    172.52.47.194   \n",
       "2              20               0.89            card     185.98.35.23   \n",
       "3               6               0.37   wire_transfer    107.136.36.87   \n",
       "4              13               0.27             ACH  108.161.108.255   \n",
       "\n",
       "  device_hash  \n",
       "0    D8536477  \n",
       "1    D2622631  \n",
       "2    D4823498  \n",
       "3    D9961380  \n",
       "4    D7637601  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(f\"SELECT * FROM read_parquet('{parquet_path}')LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6821a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1280251</td>\n",
       "      <td>2024-01-01 22:58:30.131850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T4841484</td>\n",
       "      <td>2024-01-01 22:54:21.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T2469382</td>\n",
       "      <td>2024-01-01 22:53:53.515483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T341139</td>\n",
       "      <td>2024-01-01 22:52:56.620090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T681385</td>\n",
       "      <td>2024-01-01 22:50:49.475634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999995</th>\n",
       "      <td>T3517687</td>\n",
       "      <td>2023-01-01 00:23:15.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999996</th>\n",
       "      <td>T648800</td>\n",
       "      <td>2023-01-01 00:21:19.560899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999997</th>\n",
       "      <td>T3001064</td>\n",
       "      <td>2023-01-01 00:12:48.028557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999998</th>\n",
       "      <td>T114745</td>\n",
       "      <td>2023-01-01 00:11:36.452582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999999</th>\n",
       "      <td>T4983224</td>\n",
       "      <td>2023-01-01 00:09:26.241974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        transaction_id                  timestamp\n",
       "0             T1280251 2024-01-01 22:58:30.131850\n",
       "1             T4841484 2024-01-01 22:54:21.281089\n",
       "2             T2469382 2024-01-01 22:53:53.515483\n",
       "3              T341139 2024-01-01 22:52:56.620090\n",
       "4              T681385 2024-01-01 22:50:49.475634\n",
       "...                ...                        ...\n",
       "4999995       T3517687 2023-01-01 00:23:15.259766\n",
       "4999996        T648800 2023-01-01 00:21:19.560899\n",
       "4999997       T3001064 2023-01-01 00:12:48.028557\n",
       "4999998        T114745 2023-01-01 00:11:36.452582\n",
       "4999999       T4983224 2023-01-01 00:09:26.241974\n",
       "\n",
       "[5000000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine data collection period\n",
    "# check to see if there are any rows with erroneous timestamps set in the future (> 2025)\n",
    "# check to ensure timestamp consistency format\n",
    "\n",
    "# sort timestamp column by highest to lowest, include transaction_id column (to indentify unique rows in case of located error)\n",
    "con.execute(f\"SELECT transaction_id, timestamp FROM read_parquet('{parquet_path}') ORDER BY timestamp DESC\").fetch_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897dbc93",
   "metadata": {},
   "source": [
    "The data set spans the period of one year from 2023-01-01 to 2024-01-01\n",
    "\n",
    "There are no future date values in the timestamps\n",
    "\n",
    "The timestamp column does not need to be adjusted. If column is already of type TIMESTAMP, DATETIME, or TIMESTAMPZ then the database already stores them in a consistent internal format (which in our case it is stored as TIMESTAMP type). However, if the column is stored as text (VARCHAR/CHAR) it needs to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d669d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transaction_id': 5000000,\n",
       " 'timestamp': 4999998,\n",
       " 'sender_account': 896513,\n",
       " 'receiver_account': 896639,\n",
       " 'amount': 217069,\n",
       " 'transaction_type': 4,\n",
       " 'merchant_category': 8,\n",
       " 'location': 8,\n",
       " 'device_used': 4,\n",
       " 'is_fraud': 2,\n",
       " 'fraud_type': 1,\n",
       " 'time_since_last_transaction': 4103487,\n",
       " 'spending_deviation_score': 917,\n",
       " 'velocity_score': 20,\n",
       " 'geo_anomaly_score': 101,\n",
       " 'payment_channel': 4,\n",
       " 'ip_address': 4997068,\n",
       " 'device_hash': 3835723}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for logic relating to duplicate/distinct values\n",
    "# Run a loop to query the count of distinct results for each column, and return the results in a dictionary.\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    query = f\"\"\"\n",
    "        SELECT COUNT(DISTINCT {col}) AS distinct_count\n",
    "        FROM df\n",
    "    \"\"\"\n",
    "    count = duckdb.query(query).fetchone()[0]\n",
    "    results[col] = count\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99816adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transaction_id':         transaction_id\n",
       " 0              T100000\n",
       " 1             T1000000\n",
       " 2             T1000001\n",
       " 3             T1000002\n",
       " 4             T1000003\n",
       " ...                ...\n",
       " 4999995        T999995\n",
       " 4999996        T999996\n",
       " 4999997        T999997\n",
       " 4999998        T999998\n",
       " 4999999        T999999\n",
       " \n",
       " [5000000 rows x 1 columns],\n",
       " 'timestamp':                          timestamp\n",
       " 0       2023-01-01 00:09:26.241974\n",
       " 1       2023-01-01 00:11:36.452582\n",
       " 2       2023-01-01 00:12:48.028557\n",
       " 3       2023-01-01 00:21:19.560899\n",
       " 4       2023-01-01 00:23:15.259766\n",
       " ...                            ...\n",
       " 4999993 2024-01-01 22:50:49.475634\n",
       " 4999994 2024-01-01 22:52:56.620090\n",
       " 4999995 2024-01-01 22:53:53.515483\n",
       " 4999996 2024-01-01 22:54:21.281089\n",
       " 4999997 2024-01-01 22:58:30.131850\n",
       " \n",
       " [4999998 rows x 1 columns],\n",
       " 'sender_account':        sender_account\n",
       " 0           ACC100000\n",
       " 1           ACC100001\n",
       " 2           ACC100002\n",
       " 3           ACC100003\n",
       " 4           ACC100004\n",
       " ...               ...\n",
       " 896508      ACC999995\n",
       " 896509      ACC999996\n",
       " 896510      ACC999997\n",
       " 896511      ACC999998\n",
       " 896512      ACC999999\n",
       " \n",
       " [896513 rows x 1 columns],\n",
       " 'receiver_account':        receiver_account\n",
       " 0             ACC100000\n",
       " 1             ACC100001\n",
       " 2             ACC100002\n",
       " 3             ACC100003\n",
       " 4             ACC100004\n",
       " ...                 ...\n",
       " 896634        ACC999995\n",
       " 896635        ACC999996\n",
       " 896636        ACC999997\n",
       " 896637        ACC999998\n",
       " 896638        ACC999999\n",
       " \n",
       " [896639 rows x 1 columns],\n",
       " 'amount':          amount\n",
       " 0          0.01\n",
       " 1          0.02\n",
       " 2          0.03\n",
       " 3          0.04\n",
       " 4          0.05\n",
       " ...         ...\n",
       " 217064  3228.86\n",
       " 217065  3281.96\n",
       " 217066  3411.59\n",
       " 217067  3419.97\n",
       " 217068  3520.57\n",
       " \n",
       " [217069 rows x 1 columns],\n",
       " 'transaction_type':   transaction_type\n",
       " 0          deposit\n",
       " 1          payment\n",
       " 2         transfer\n",
       " 3       withdrawal,\n",
       " 'merchant_category':   merchant_category\n",
       " 0     entertainment\n",
       " 1           grocery\n",
       " 2            online\n",
       " 3             other\n",
       " 4        restaurant\n",
       " 5            retail\n",
       " 6            travel\n",
       " 7         utilities,\n",
       " 'location':     location\n",
       " 0     Berlin\n",
       " 1      Dubai\n",
       " 2     London\n",
       " 3   New York\n",
       " 4  Singapore\n",
       " 5     Sydney\n",
       " 6      Tokyo\n",
       " 7    Toronto,\n",
       " 'device_used':   device_used\n",
       " 0         atm\n",
       " 1      mobile\n",
       " 2         pos\n",
       " 3         web,\n",
       " 'is_fraud':    is_fraud\n",
       " 0     False\n",
       " 1      True,\n",
       " 'fraud_type':          fraud_type\n",
       " 0  card_not_present\n",
       " 1              None,\n",
       " 'time_since_last_transaction':          time_since_last_transaction\n",
       " 0                       -8777.814182\n",
       " 1                       -8759.574443\n",
       " 2                       -8759.511666\n",
       " 3                       -8759.095447\n",
       " 4                       -8753.662225\n",
       " ...                              ...\n",
       " 4103483                  8751.006254\n",
       " 4103484                  8753.470631\n",
       " 4103485                  8757.627835\n",
       " 4103486                  8757.758483\n",
       " 4103487                          NaN\n",
       " \n",
       " [4103488 rows x 1 columns],\n",
       " 'spending_deviation_score':      spending_deviation_score\n",
       " 0                       -5.26\n",
       " 1                       -5.04\n",
       " 2                       -5.03\n",
       " 3                       -4.84\n",
       " 4                       -4.83\n",
       " ..                        ...\n",
       " 912                      4.66\n",
       " 913                      4.71\n",
       " 914                      4.85\n",
       " 915                      4.90\n",
       " 916                      5.02\n",
       " \n",
       " [917 rows x 1 columns],\n",
       " 'velocity_score':     velocity_score\n",
       " 0                1\n",
       " 1                2\n",
       " 2                3\n",
       " 3                4\n",
       " 4                5\n",
       " 5                6\n",
       " 6                7\n",
       " 7                8\n",
       " 8                9\n",
       " 9               10\n",
       " 10              11\n",
       " 11              12\n",
       " 12              13\n",
       " 13              14\n",
       " 14              15\n",
       " 15              16\n",
       " 16              17\n",
       " 17              18\n",
       " 18              19\n",
       " 19              20,\n",
       " 'geo_anomaly_score':      geo_anomaly_score\n",
       " 0                 0.00\n",
       " 1                 0.01\n",
       " 2                 0.02\n",
       " 3                 0.03\n",
       " 4                 0.04\n",
       " ..                 ...\n",
       " 96                0.96\n",
       " 97                0.97\n",
       " 98                0.98\n",
       " 99                0.99\n",
       " 100               1.00\n",
       " \n",
       " [101 rows x 1 columns],\n",
       " 'payment_channel':   payment_channel\n",
       " 0             ACH\n",
       " 1             UPI\n",
       " 2            card\n",
       " 3   wire_transfer,\n",
       " 'ip_address':            ip_address\n",
       " 0         0.0.102.150\n",
       " 1         0.0.104.230\n",
       " 2          0.0.11.193\n",
       " 3         0.0.111.187\n",
       " 4         0.0.114.224\n",
       " ...               ...\n",
       " 4997063  99.99.86.217\n",
       " 4997064  99.99.92.111\n",
       " 4997065  99.99.93.231\n",
       " 4997066   99.99.95.88\n",
       " 4997067   99.99.97.34\n",
       " \n",
       " [4997068 rows x 1 columns],\n",
       " 'device_hash':         device_hash\n",
       " 0          D1000002\n",
       " 1          D1000003\n",
       " 2          D1000006\n",
       " 3          D1000007\n",
       " 4          D1000012\n",
       " ...             ...\n",
       " 3835718    D9999993\n",
       " 3835719    D9999994\n",
       " 3835720    D9999995\n",
       " 3835721    D9999998\n",
       " 3835722    D9999999\n",
       " \n",
       " [3835723 rows x 1 columns]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for logic relating to duplicate/distinct values\n",
    "# Run a loop to query the distinct values for each column, and return the results in a dictionary.\n",
    "\n",
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT {col}\n",
    "        FROM read_parquet('{parquet_path}')\n",
    "        ORDER BY {col}\n",
    "    \"\"\"\n",
    "    unique_values[col] = con.execute(query).fetch_df()\n",
    "\n",
    "unique_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7aff5",
   "metadata": {},
   "source": [
    "#### **Assesment of distinct values by column:**\n",
    "\n",
    "**transaction_id:** 5 million unique values, this is logical as there are 5 million rows and each transaction should have its own unique value.\n",
    "\n",
    "**timestamp:** 4,999,998 unique values, no nulls values so that means there is two transactions that occurred at the same time as other transactions. To be broken down into month, day of week, hour during feature engineering.\n",
    "\n",
    "**sender_account:** 896,513 unique values (may be hashed for PI reasons)\n",
    "\n",
    "**receiver_account:** 896639 unique values (may be hashed for PI reaons)\n",
    "\n",
    "**amount:** 217,068 unique values that range from 0.01 to 3520.57. We may want to consider converting amount into ranges or categories of some sort when feature engineering.\n",
    "\n",
    "**transaction_type:** 4 unique values; deposit, payment, transfer, withdrawal\n",
    "\n",
    "**merchant_category:** 8 unique values; entertainment, grocery, online, other, restaurant, retail, travel, utilities\n",
    "\n",
    "**location:** 8 unique values; Berlin, Dubai, London, New York, Singapore, Sydney, Tokyo, Toronto\n",
    "\n",
    "**device_used:** 4 unique values;  atm, mobile, pos, web\n",
    "\n",
    "**is-fraud:** 2 unique values; 0 = false and 1 = true\n",
    "\n",
    "**fraud_type:** 2 unique values; card_not_present and none. This column offers little value - to be deleted.\n",
    "\n",
    "**time_since_last transaction:** 4,103,488 unique values. Ranges from -8777.814182 to 8757.758483 We may want to convert into range or categories of some sort when feature engineering (ex. less than one minute, less than 5 minutes etc).\n",
    "\n",
    "**spending_deviation_score:** 917 unique values; raning from -5.26 to 5.02\n",
    "\n",
    "**velocity_score:** 20 unique values; ranges from 1-20\n",
    "\n",
    "**geo_anomaly_score:** 101 unique values; ranges from 0-1 (decimal values)\n",
    "\n",
    "**payment_channel:** 4 unique values;  ACH, UPI, card, wire_transfer\n",
    "\n",
    "**ip_address:** 4,997,068 unique values (to be hashed for PI reasons)\n",
    "\n",
    "**device_hash:** 3,835,723 unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  payment_channel  is_fraud_count  not_fraud_count\n",
      "0   wire_transfer           45034          1206185\n",
      "1            card           44885          1204808\n",
      "2             UPI           44896          1203951\n",
      "3             ACH           44738          1205503\n"
     ]
    }
   ],
   "source": [
    "# Count of true and false fraud values by payment channel type\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        payment_channel,\n",
    "        COUNT(*) FILTER (WHERE is_fraud = '1') AS is_fraud_count,\n",
    "        COUNT(*) FILTER (WHERE is_fraud = '0') AS not_fraud_count\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    GROUP BY payment_channel\n",
    "    ORDER BY payment_channel DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = con.execute(query).fetch_df()\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03aacdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_fraud_count  not_fraud_count\n",
      "0               0           896513\n"
     ]
    }
   ],
   "source": [
    "# Count of true and false fraud values by payment channel type\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) FILTER (WHERE is_fraud = '1' AND time_since_last_transaction IS NULL) AS is_fraud_count,\n",
    "        COUNT(*) FILTER (WHERE is_fraud = '0' AND time_since_last_transaction IS NULL ) AS not_fraud_count\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "\"\"\"\n",
    "\n",
    "result_df = con.execute(query).fetch_df()\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1cbcb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is_fraud\n",
      "0    896513\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "    COUNT (*) is_fraud\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE time_since_last_transaction IS NULL\n",
    "\"\"\"\n",
    "\n",
    "result_df = con.execute(query).fetch_df()\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadf803",
   "metadata": {},
   "source": [
    "## Data Exploration - TODO\n",
    "\n",
    "- Get number of columns, column names, column names and data types.\n",
    "- Check for type mismatches (e.g numeric stored as text)\n",
    "- Check for rows with missing values/NA\n",
    "- Check columns with MV/NA\n",
    "- Check ratio of fraud:non-fraud cases\n",
    "* If such rows (missing values, NA, null) are excluded how many rows would be left\n",
    "* Check for duplicates, \n",
    "* Outliers, \n",
    "* Timestamp consistency, \n",
    "* Class imbalance, data leakage/PII checks, and downstream sample sizes after each filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf688b7b",
   "metadata": {},
   "source": [
    "##NOTES ON EDA:\n",
    "\n",
    "- Row/column completeness impact ‚Äî Compute how many rows remain after dropping rows with any NA and after dropping only rows missing critical fields (e.g., is_fraud, amount) so you can plan sample sizes for training\n",
    "\n",
    "- Class imbalance and sampling ‚Äî Measure fraud:non‚Äëfraud ratio and per‚Äëgroup rates (by merchant, device, country). This informs evaluation metrics and resampling strategies (class weights, SMOTE, stratified sampling)\n",
    "\n",
    "- Duplicates and identity checks ‚Äî Look for duplicate transaction_id or repeated (sender, receiver, timestamp, amount) tuples. Duplicates can bias counts and model training\n",
    "\n",
    "- Outliers and distributions ‚Äî Inspect amount, time_since_last_transaction, and anomaly scores for extreme values and skew. Decide winsorizing, log transforms, or robust scaling. Visualize with histograms or quantile summaries.\n",
    "\n",
    "- Timestamp and temporal integrity ‚Äî Check for timezone issues, future dates, or inconsistent formats. Verify monotonicity for per‚Äëaccount sequences if we‚Äôll use time‚Äëbased features.\n",
    "\n",
    "- For merchant_category, location, device_used, check unique counts and frequency tails. Rare categories may need grouping into ‚Äúother‚Äù or target encoding.\n",
    "\n",
    "- Compute correlation matrix for numeric features and check for highly correlated predictors that may harm some models.\n",
    "\n",
    "- Validate is_fraud and fraud_type consistency; ensure no features leak the label (e.g., fraud_flag derived from is_fraud). Check that features available at prediction time won‚Äôt include future info.\n",
    "\n",
    "- PII and privacy ‚Äî Identify columns with PII (account IDs, IPs, device hashes). Decide hashing/anonymization and access controls before sharing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e763a",
   "metadata": {},
   "source": [
    "## Next We find patterns and relationship in the Dataset\n",
    "- Find patterns and relationships ‚Äî bivariate analysis, correlations, time‚Äëseries patterns per account, and group‚Äëlevel fraud rates.\n",
    "- Run feature importance checks to see which features/variables are important or critical to the target variable (is_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99260ca0",
   "metadata": {},
   "source": [
    "# Then we proceed to Feature Engineering\n",
    "- Feature Engineering is the process of creating new, more informative columns (features) from our raw data to help machine learning models detect patterns better.\n",
    "\n",
    "- A model looking at raw transaction data might miss subtle fraud patterns. But engineered features can make those patterns obvious.\n",
    "\n",
    "## Examples\n",
    "hour_of_day (from timestamp)\n",
    "\n",
    "is_weekend (1 if Saturday/Sunday)\n",
    "\n",
    "log_amount (logarithm of transaction amount)\n",
    "\n",
    "merchant_risk_score (categorize merchants as high/medium/low risk)\n",
    "\n",
    "amount_deviation = (amount - customer_avg_amount) / customer_avg_amount"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
