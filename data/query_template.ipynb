{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c420dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import duckdb\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b8c39",
   "metadata": {},
   "source": [
    "### Use the following bash command to find the cached file path\n",
    "find ~ -name \"financial_fraud_detection_dataset.csv\"\n",
    "\n",
    "If you're not able to, then download a copy of the dataset to your machine, unzip it and set the absolute path of the csv file in the next cell.\n",
    "\n",
    "- Download link:\n",
    "https://www.kaggle.com/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the file path of the cached dataset below to read it into a pandas DataFrame\n",
    "df = pd.read_csv(\"/Users/joshuaokojie/.cache/kagglehub/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/versions/1/financial_fraud_detection_dataset.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19739dd1",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleaning Using SQL Queries in DuckDB\n",
    "\n",
    "METHOD:\n",
    "- Using a Local SQL Engine (DuckDB)\n",
    "    - For complex SQL queries, loading our data into a local analytical database like DuckDB is very effective. It's fast and supports direct querying on Pandas DataFrames or CSV files.\n",
    "    - We can use DuckDB to query CSV/parquet file directly and perform the filtering in SQL, which is more memory efficient.\n",
    "    - DuckDB is optimized for analytical queries and can be faster than pandas for complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea310a",
   "metadata": {},
   "source": [
    "   ** Workflow**\n",
    "   - \n",
    "    - Download dataset to local machine\n",
    "    - connect to path in jupyter notebook, and convert csv to parquet files (columnar Parquet files that are much faster to query)\n",
    "    - Store parquet files in folder within the repo (parquet files are smaller)\n",
    "    - Run sql queries directly on he parquet files without importing them into memory\n",
    "    - Feature Engineering (DuckDB SQL) or Pandas\n",
    "    - Saved clean and processed parquet shards/files to be used in other notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "csv_path = \"/Users/joshuaokojie/.cache/kagglehub/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/versions/1/financial_fraud_detection_dataset.csv\"\n",
    "parquet_path = \"./raw_data/financial_fraud_detection_dataset.parquet\"\n",
    "cleaned_parquet_path = \"./cleaned_data/cleaned_fraud.parquet\"\n",
    "\n",
    "# 1. Check if source CSV exists\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {csv_path}\")\n",
    "\n",
    "print(f\"üìÅ Source CSV: {csv_path}\")\n",
    "print(f\"üìÅ Target Parquet: {parquet_path}\")\n",
    "print(f\"üìä Original size: {os.path.getsize(csv_path) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16524a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Parquet size: 0.19 GB\n",
      "üéØ Compression ratio: 74.6% reduction\n",
      "üî¢ Row count in Parquet: 5,000,000\n"
     ]
    }
   ],
   "source": [
    "# Connect to duckdb and Convert CSV to Parquet with DuckDB\n",
    "\n",
    "# CREATE THE DIRECTORY parquet_path directory FIRST\n",
    "os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"\"\"\n",
    "COPY (SELECT * FROM read_csv_auto('{csv_path}'))\n",
    "TO '{parquet_path}' (FORMAT 'parquet', COMPRESSION 'zstd');\n",
    "\"\"\")\n",
    "\n",
    "#  3. Verify the result\n",
    "if os.path.exists(parquet_path):\n",
    "    parquet_size = os.path.getsize(parquet_path) / (1024**3)\n",
    "    compression_ratio = (1 - parquet_size / (os.path.getsize(csv_path) / (1024**3))) * 100\n",
    "    print(f\"üìä Parquet size: {parquet_size:.2f} GB\")\n",
    "    print(f\"üéØ Compression ratio: {compression_ratio:.1f}% reduction\")\n",
    "    \n",
    "    # Quick verification query\n",
    "    row_count = con.execute(f\"SELECT COUNT(*) FROM '{parquet_path}'\").fetchone()[0]\n",
    "    print(f\"üî¢ Row count in Parquet: {row_count:,}\")\n",
    "else:\n",
    "    print(\"‚ùå Parquet file was not created\")\n",
    "\n",
    "# To Close DB connection, but can be left open for further queries\n",
    "# con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = duckdb.connect()\n",
    "\n",
    "# using DESCRIBE instead of pandas dtypes to avoid loading data into memory\n",
    "print(con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{parquet_path}')\").fetch_df())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadf803",
   "metadata": {},
   "source": [
    "## Data Exploration - TODO\n",
    "\n",
    "- Get number of columns, column names, column names and data types.\n",
    "- Check for type mismatches (e.g numeric stored as text)\n",
    "- Check for rows with missing values/NA\n",
    "- Check columns with MV/NA\n",
    "- Check ratio of fraud:non-fraud cases\n",
    "* If such rows (missing values, NA, null) are excluded how many rows would be left\n",
    "* Check for duplicates, \n",
    "* Outliers, \n",
    "* Timestamp consistency, \n",
    "* Class imbalance, data leakage/PII checks, and downstream sample sizes after each filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf688b7b",
   "metadata": {},
   "source": [
    "##NOTES ON EDA:\n",
    "\n",
    "- Row/column completeness impact ‚Äî Compute how many rows remain after dropping rows with any NA and after dropping only rows missing critical fields (e.g., is_fraud, amount) so you can plan sample sizes for training\n",
    "\n",
    "- Class imbalance and sampling ‚Äî Measure fraud:non‚Äëfraud ratio and per‚Äëgroup rates (by merchant, device, country). This informs evaluation metrics and resampling strategies (class weights, SMOTE, stratified sampling)\n",
    "\n",
    "- Duplicates and identity checks ‚Äî Look for duplicate transaction_id or repeated (sender, receiver, timestamp, amount) tuples. Duplicates can bias counts and model training\n",
    "\n",
    "- Outliers and distributions ‚Äî Inspect amount, time_since_last_transaction, and anomaly scores for extreme values and skew. Decide winsorizing, log transforms, or robust scaling. Visualize with histograms or quantile summaries.\n",
    "\n",
    "- Timestamp and temporal integrity ‚Äî Check for timezone issues, future dates, or inconsistent formats. Verify monotonicity for per‚Äëaccount sequences if we‚Äôll use time‚Äëbased features.\n",
    "\n",
    "- For merchant_category, location, device_used, check unique counts and frequency tails. Rare categories may need grouping into ‚Äúother‚Äù or target encoding.\n",
    "\n",
    "- Compute correlation matrix for numeric features and check for highly correlated predictors that may harm some models.\n",
    "\n",
    "- Validate is_fraud and fraud_type consistency; ensure no features leak the label (e.g., fraud_flag derived from is_fraud). Check that features available at prediction time won‚Äôt include future info.\n",
    "\n",
    "- PII and privacy ‚Äî Identify columns with PII (account IDs, IPs, device hashes). Decide hashing/anonymization and access controls before sharing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e763a",
   "metadata": {},
   "source": [
    "## Next We find patterns and relationship in the Dataset\n",
    "- Find patterns and relationships ‚Äî bivariate analysis, correlations, time‚Äëseries patterns per account, and group‚Äëlevel fraud rates.\n",
    "- Run feature importance checks to see which features/variables are important or critical to the target variable (is_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99260ca0",
   "metadata": {},
   "source": [
    "# Then we proceed to Feature Engineering\n",
    "- Feature Engineering is the process of creating new, more informative columns (features) from our raw data to help machine learning models detect patterns better.\n",
    "\n",
    "- A model looking at raw transaction data might miss subtle fraud patterns. But engineered features can make those patterns obvious.\n",
    "\n",
    "## Examples\n",
    "hour_of_day (from timestamp)\n",
    "\n",
    "is_weekend (1 if Saturday/Sunday)\n",
    "\n",
    "log_amount (logarithm of transaction amount)\n",
    "\n",
    "merchant_risk_score (categorize merchants as high/medium/low risk)\n",
    "\n",
    "amount_deviation = (amount - customer_avg_amount) / customer_avg_amount"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
