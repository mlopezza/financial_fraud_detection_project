{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36959e64",
      "metadata": {},
      "source": [
        "# Model 1 extension and modification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "217fb561",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import os\n",
        "import duckdb\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0065f183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Original size: 0.14 GB\n"
          ]
        }
      ],
      "source": [
        "cleaned_parquet = \"../data/cleaned_data/cleaned_fraud.parquet\"\n",
        "\n",
        "print(f\"ðŸ“Š Original size: {os.path.getsize(cleaned_parquet) / (1024**3):.2f} GB\")\n",
        "\n",
        "con = duckdb.connect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6cafd824",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Class Distribution:\n",
            "is_fraud\n",
            "False    0.956244\n",
            "True     0.043756\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Rows with negative time_since_last_transaction: 2051331\n",
            "Negative values might indicate data errors or specific flags. Treating as valid numeric for now.\n"
          ]
        }
      ],
      "source": [
        "# Load full dataset\n",
        "df = con.execute(f\"SELECT * FROM '{cleaned_parquet}'\").fetch_df()\n",
        "\n",
        "# 1. EDA & Pattern Detection\n",
        "print(\"Original Class Distribution:\")\n",
        "print(df['is_fraud'].value_counts(normalize=True))\n",
        "\n",
        "# Check for negative time values (Pattern Detection)\n",
        "neg_time = df[df['time_since_last_transaction'] < 0]\n",
        "print(f\"\\nRows with negative time_since_last_transaction: {len(neg_time)}\")\n",
        "if len(neg_time) > 0:\n",
        "    print(\"Negative values might indicate data errors or specific flags. Treating as valid numeric for now.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0c1210f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339bc90f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 2. Data Preparation for SMOTE\n",
        "# - Drop high cardinality identifiers\n",
        "# - Encode categorical variables\n",
        "# - Scale numerical variables\n",
        "\n",
        "categorical_cols = ['transaction_type', 'merchant_category', 'location', 'device_used', 'payment_channel']\n",
        "numerical_cols = ['amount', 'time_since_last_transaction', 'spending_deviation_score', 'velocity_score', 'geo_anomaly_score', 'hour', 'day_of_week']\n",
        "drop_cols = ['sender_account', 'receiver_account', 'ip_address', 'device_hash', 'year', 'month', 'day_of_month'] # high-cardinality identifiers that do not generalize well and can negatively affect the model training. Dropping date because we already extracted more meaningful features like hours and days of the week\n",
        "\n",
        "# Separate Features and Target\n",
        "X = df.drop(columns=['is_fraud'] + drop_cols, errors='ignore')\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split Data (Best Practice: Split BEFORE SMOTE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # stratifying by y to ensure both splits have an equal amount of the target variable y\n",
        "\n",
        "# Preprocessing Pipeline \n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4b59bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next Steps\n",
        "# After Smote, we can train and evaluate multiple models on the training and test (no-smote) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc028d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Full Pipeline: Preprocess â†’ SMOTE â†’ Model - automatic preprocessing applying SMOTE in CV\n",
        "smote_pipeline = ImbPipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42)),\n",
        "    (\"model\", LogisticRegression(max_iter=1000, n_jobs=-1))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2705c34f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run CROSS VALIDATION - this is training and evaluation loop\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(\n",
        "    smote_pipeline,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(scores.mean(), scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab269c59",
      "metadata": {},
      "source": [
        "Cross-validation â†’ choose best model â†’ final training â†’ test evaluation\n",
        "CV only evaluates the pipeline configuration.\n",
        "`.fit()` trains the final model using that configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcce4927",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model\n",
        "smote_pipeline.fit(X_train, y_train) # train on the full training set\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "y_pred = smote_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac8495c",
      "metadata": {},
      "source": [
        "# Using Other ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f5d7ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "rf_pipeline = ImbPipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42)),\n",
        "    (\"model\", RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6ff466",
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "xgb_pipeline = ImbPipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=42)),\n",
        "    (\"model\", XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"logloss\",\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7134b468",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in {\n",
        "    \"RandomForest\": rf_pipeline,\n",
        "    \"XGBoost\": xgb_pipeline\n",
        "}.items():\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1)\n",
        "    print(f\"{name}: mean F1={scores.mean():.4f}, std={scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b43794",
      "metadata": {},
      "outputs": [],
      "source": [
        "# After which we can then train the models\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "# OR \n",
        "# rf_pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c67b3da",
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_pipeline.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c4a8312",
      "metadata": {},
      "source": [
        "#  feature importance across Logistic Regression, RandomForest, and XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f056cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_names(preprocessor):\n",
        "    num_features = preprocessor.named_transformers_['num'].get_feature_names_out()\n",
        "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
        "    return list(num_features) + list(cat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85673482",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = get_feature_names(smote_pipeline.named_steps[\"preprocess\"])\n",
        "feature_names = get_feature_names(rf_pipeline.named_steps[\"preprocess\"])\n",
        "feature_names = get_feature_names(xgb_pipeline.named_steps[\"preprocess\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31be948f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "logreg = smote_pipeline.named_steps[\"model\"]\n",
        "\n",
        "logreg_importance = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": np.abs(logreg.coef_[0])\n",
        "}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "# (LogReg uses coefficients)\n",
        "# â€¢ Large positive coefficient â†’ increases fraud probability\n",
        "# â€¢ Large negative coefficient â†’ decreases fraud probability\n",
        "# â€¢ Logistic Regression captures linear relationships only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d5f8ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (RandomForest uses Gini importance)\n",
        "# â€¢ Measures how much each feature reduces impurity\n",
        "# â€¢ Captures nonlinear interactions\n",
        "# â€¢ Tends to favor highâ€‘cardinality oneâ€‘hot encoded features\n",
        "\n",
        "rf_importance = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": smote_pipeline.named_steps[\"model\"].feature_importances_\n",
        "}).sort_values(\"importance\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128168ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost uses gain, the most meaningful metric\n",
        "# Gain = how much a feature improves splits\n",
        "# â€¢ XGBoost captures complex nonlinear patterns\n",
        "# â€¢ Often the most reliable importance measure for fraud\n",
        "\n",
        "xgb_model = smote_pipeline.named_steps[\"model\"]\n",
        "\n",
        "xgb_importance = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": xgb_model.get_booster().get_score(importance_type=\"gain\")\n",
        "}).sort_values(\"importance\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c866980e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all three into a comparison table\n",
        "\n",
        "comparison = (\n",
        "    logreg_importance.rename(columns={\"importance\": \"logreg\"})\n",
        "    .merge(rf_importance.rename(columns={\"importance\": \"rf\"}), on=\"feature\", how=\"outer\")\n",
        "    .merge(xgb_importance.rename(columns={\"importance\": \"xgb\"}), on=\"feature\", how=\"outer\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "comparison.sort_values(\"xgb\", ascending=False).head(20)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dsi_participant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
