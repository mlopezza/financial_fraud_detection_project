{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff699b09",
   "metadata": {},
   "source": [
    "# Focused on heavily imbalanced data\n",
    "1. Use stratified train/test splits to preserve fraud ratio in both sets.\n",
    "2. Apply SMOTE only to the training set — never to the test set.\n",
    "3. Use F1, precision, recall as evaluation metrics, not just accuracy.\n",
    "4. Try both SMOTE and class_weight='balanced' in logistic regression to compare which handles imbalance better.\n",
    "5. Threshold tuning, using the default 0.5 cutoff which may not be optimal for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c15da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1b: imports (some you already have, but safe to re-run)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load cleaned dataset if needed\n",
    "\n",
    "cleaned_parquet = \"../data/cleaned_data/cleaned_fraud.parquet\"\n",
    "\n",
    "df = pd.read_parquet(cleaned_parquet)\n",
    "\n",
    "print(df.shape)\n",
    "print(df['is_fraud'].value_counts())\n",
    "print(df['is_fraud'].value_counts(normalize=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: define features (X) and target (y)\n",
    "\n",
    "target_col = \"is_fraud\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col].astype(int)   # convert True/False -> 1/0\n",
    "\n",
    "# Define numeric and categorical features based on your column list\n",
    "numeric_features = [\n",
    "    \"amount\",\n",
    "    \"time_since_last_transaction\",\n",
    "    \"spending_deviation_score\",\n",
    "    \"velocity_score\",\n",
    "    \"geo_anomaly_score\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_month\",\n",
    "    \"hour\",\n",
    "    \"day_of_week\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sender_account\",\n",
    "    \"receiver_account\",\n",
    "    \"transaction_type\",\n",
    "    \"merchant_category\",\n",
    "    \"location\",\n",
    "    \"device_used\",\n",
    "    \"payment_channel\",\n",
    "    \"ip_address\",\n",
    "    \"device_hash\",\n",
    "]\n",
    "\n",
    "# Keep only those that actually exist in X\n",
    "numeric_features = [c for c in numeric_features if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]\n",
    "\n",
    "numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b275d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: stratified train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "print(\"Train fraud ratio:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Test fraud ratio:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0031c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: preprocessing pipelines\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: SMOTE + logistic regression pipeline\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "smote_pipeline = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"smote\", smote),      # applied only on training data inside CV / fit\n",
    "        (\"model\", log_reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "smote_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77040d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: logistic regression with class_weight='balanced' (no SMOTE)\n",
    "\n",
    "log_reg_balanced = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "balanced_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", log_reg_balanced),\n",
    "    ]\n",
    ")\n",
    "\n",
    "balanced_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: cross-validation comparison (SMOTE vs class_weight)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scorers = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"f1\": f1_score,\n",
    "}\n",
    "\n",
    "def evaluate_cv(pipeline, X_train, y_train, cv, scorers, label):\n",
    "    print(f\"\\n=== CV results for {label} ===\")\n",
    "    for name, func in scorers.items():\n",
    "        scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=make_scorer(func),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        print(f\"{name}: mean={scores.mean():.3f}, std={scores.std():.3f}\")\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "evaluate_cv(smote_pipeline, X_train, y_train, cv, scorers, label=\"SMOTE + LR\")\n",
    "evaluate_cv(balanced_pipeline, X_train, y_train, cv, scorers, label=\"class_weight='balanced' + LR\")\n",
    "\n",
    "# We'd pick whichever gives better recall/F1 on the fraud class; suppose you choose SMOTE for the next steps (you can swap easily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: fit chosen model (here SMOTE pipeline) and evaluate on test set\n",
    "\n",
    "chosen_pipeline = smote_pipeline   # or balanced_pipeline if it performed better\n",
    "\n",
    "chosen_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = chosen_pipeline.predict(X_test)\n",
    "y_proba = chosen_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Classification report (threshold = 0.5) ===\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-fraud\", \"Fraud\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - threshold 0.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5344d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: threshold tuning\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 17)  # 0.1, 0.15, ..., 0.9\n",
    "records = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (y_proba >= thr).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_thr)\n",
    "    rec = recall_score(y_test, y_pred_thr)\n",
    "    f1 = f1_score(y_test, y_pred_thr)\n",
    "    acc = accuracy_score(y_test, y_pred_thr)\n",
    "    records.append((thr, prec, rec, f1, acc))\n",
    "\n",
    "thr_df = pd.DataFrame(records, columns=[\"threshold\", \"precision\", \"recall\", \"f1\", \"accuracy\"])\n",
    "thr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19114be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: plot metrics vs threshold\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"recall\"], label=\"Recall\")\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"f1\"], label=\"F1\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision / Recall / F1 vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ccf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: final evaluation at chosen threshold\n",
    "\n",
    "best_thr = 0.30  # set this based on thr_df / plot\n",
    "\n",
    "y_pred_best = (y_proba >= best_thr).astype(int)\n",
    "\n",
    "print(f\"=== Classification report (threshold = {best_thr}) ===\\n\")\n",
    "print(classification_report(y_test, y_pred_best, digits=4))\n",
    "\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=[\"Non-fraud\", \"Fraud\"])\n",
    "disp_best.plot(cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix - threshold {best_thr}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f73f2",
   "metadata": {},
   "source": [
    "# ROC & PR (Precision-Recall) Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47aa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A: ROC Curve for chosen model\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve values\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Chosen Model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Add interpretation for ROC/PR curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B: Precision-Recall Curve\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "avg_precision = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, lw=2, color=\"purple\", label=f\"AP = {avg_precision:.4f}\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve — Chosen Model\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
