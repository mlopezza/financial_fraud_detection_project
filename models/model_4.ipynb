{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86556e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost imbalanced-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3087566",
   "metadata": {},
   "source": [
    "# Tree‑Based Models With SMOTE + Preprocessing (Full Notebook Cells)\n",
    "Here: We’ll build:\n",
    "1. RandomForest + SMOTE\n",
    "2. XGBoost + SMOTE (optional but recommended)\n",
    "3. Cross‑validation comparison\n",
    "4. Test‑set evaluation\n",
    "5. ROC + PR curves\n",
    "6. Side‑by‑side comparison with your logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1b: imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "#####\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "###\n",
    "# from sklearn.base import accuracy_score\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739434c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "cleaned_parquet = \"../data/cleaned_data/cleaned_fraud.parquet\"\n",
    "\n",
    "df = pd.read_parquet(cleaned_parquet)\n",
    "\n",
    "X = df.drop(columns=[\"is_fraud\"])\n",
    "y = df[\"is_fraud\"].astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipelines\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "numeric_features = [\n",
    "    \"amount\", \"time_since_last_transaction\", \"spending_deviation_score\",\n",
    "    \"velocity_score\", \"geo_anomaly_score\", \"year\", \"month\",\n",
    "    \"day_of_month\", \"hour\", \"day_of_week\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"sender_account\", \"receiver_account\", \"transaction_type\",\n",
    "    \"merchant_category\", \"location\", \"device_used\",\n",
    "    \"payment_channel\", \"ip_address\", \"device_hash\"\n",
    "]\n",
    "\n",
    "numeric_features = [c for c in numeric_features if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# RandomForest + SMOTE pipeline\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_pipeline = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", rf_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a889f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost + SMOTE pipeline\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_pipeline = ImbPipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", xgb_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression + SMOTE\n",
    "smote_pipeline = ImbPipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# RandomForest + SMOTE\n",
    "rf_pipeline = ImbPipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300, n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# XGBoost + SMOTE\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\", n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73243b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with cross-validation (RF vs XGB vs Logistic Regression)\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"LogReg + SMOTE\": smote_pipeline,\n",
    "    \"RandomForest + SMOTE\": rf_pipeline,\n",
    "    \"XGBoost + SMOTE\": xgb_pipeline,\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_model(name, pipeline):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    for metric_name, scorer in {\n",
    "        \"precision\": precision_score,\n",
    "        \"recall\": recall_score,\n",
    "        \"f1\": f1_score,\n",
    "        \"accuracy\": accuracy_score,\n",
    "    }.items():\n",
    "        scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=make_scorer(scorer),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        print(f\"{metric_name}: mean={scores.mean():.4f}, std={scores.std():.4f}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    evaluate_model(name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best tree model on full training data\n",
    "\n",
    "chosen_tree = xgb_pipeline   # or rf_pipeline\n",
    "\n",
    "chosen_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = chosen_tree.predict(X_test)\n",
    "y_proba_tree = chosen_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred_tree, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_tree)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-fraud\", \"Fraud\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix — Tree Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_tree)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color=\"darkorange\")\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Tree Model\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfdcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision‑Recall Curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba_tree)\n",
    "avg_precision = average_precision_score(y_test, y_proba_tree)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall, precision, label=f\"AP = {avg_precision:.4f}\", color=\"purple\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve — Tree Model\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC/PR of Logistic Regression vs Tree Model\n",
    "\n",
    "# Logistic regression probabilities\n",
    "y_proba_lr = smote_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC curves\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_proba_tree)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"LogReg + SMOTE\")\n",
    "plt.plot(fpr_tree, tpr_tree, label=\"Tree Model + SMOTE\")\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# PR curves\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_proba_lr)\n",
    "precision_tree, recall_tree, _ = precision_recall_curve(y_test, y_proba_tree)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall_lr, precision_lr, label=\"LogReg + SMOTE\")\n",
    "plt.plot(recall_tree, precision_tree, label=\"Tree Model + SMOTE\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85765b12",
   "metadata": {},
   "source": [
    "# Feature importance plots\n",
    "Tree models naturally learn nonlinear interactions, and feature importance plots help us understand which signals actually drive fraud predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Fit RandomForest pipeline\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract the trained RandomForest model\n",
    "rf_model = rf_pipeline.named_steps[\"model\"]\n",
    "\n",
    "# Extract feature names after preprocessing\n",
    "ohe = rf_pipeline.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_feature_names = np.concatenate([numeric_features, ohe_feature_names])\n",
    "\n",
    "len(all_feature_names), rf_model.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39831b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: RandomForest feature importance plot\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.barplot(data=feat_imp.head(25), x=\"importance\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 25 Feature Importances — RandomForest\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feat_imp.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1790eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fit XGBoost pipeline\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract trained XGBoost model\n",
    "xgb_model = xgb_pipeline.named_steps[\"model\"]\n",
    "\n",
    "# Reuse the same feature names from earlier\n",
    "# (numeric_features + OHE-expanded categorical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: XGBoost feature importance plot\n",
    "\n",
    "xgb_importances = xgb_model.feature_importances_\n",
    "\n",
    "xgb_feat_imp = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"importance\": xgb_importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.barplot(data=xgb_feat_imp.head(25), x=\"importance\", y=\"feature\", palette=\"magma\")\n",
    "plt.title(\"Top 25 Feature Importances — XGBoost\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "xgb_feat_imp.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
